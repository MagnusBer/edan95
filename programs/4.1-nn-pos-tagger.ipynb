{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging using Feedforward Networks\n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'rmsprop'\n",
    "SCALER = True\n",
    "SIMPLE_MODEL = False\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "MINI_CORPUS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll2009_pos():\n",
    "    train_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-train-pos.txt'\n",
    "    dev_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-development-pos.txt'\n",
    "    test_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-test-words-pos.txt'\n",
    "    # test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2009_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Corpus in a Dictionary\n",
    "We follow the fit-transform pattern of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "if MINI_CORPUS:\n",
    "    train_dict = train_dict[:len(train_dict) // 5]\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "print('First sentence, train:', train_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Context and Dictorizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDictorizer():\n",
    "    \"\"\"\n",
    "    Extract contexts of words in a sequence\n",
    "    Contexts are of w_size to the left and to the right\n",
    "    Builds an X matrix in the form of a dictionary\n",
    "    and possibly extracts the output, y, if not in the test step\n",
    "    If the test_step is True, returns y = []\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='form', output='pos', w_size=2, tolower=True):\n",
    "        self.BOS_symbol = '__BOS__'\n",
    "        self.EOS_symbol = '__EOS__'\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.w_size = w_size\n",
    "        self.tolower = tolower\n",
    "        # This was not correct as the names were not sorted\n",
    "        # self.feature_names = [input + '_' + str(i)\n",
    "        #                     for i in range(-w_size, w_size + 1)]\n",
    "        # To be sure the names are ordered\n",
    "        zeros = math.ceil(math.log10(2 * w_size + 1))\n",
    "        self.feature_names = [input + '_' + str(i).zfill(zeros) for \n",
    "                              i in range(2 * w_size + 1)]\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"\n",
    "        Build the padding rows\n",
    "        :param sentences:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.column_names = sentences[0][0].keys()\n",
    "        start = [self.BOS_symbol] * len(self.column_names)\n",
    "        end = [self.EOS_symbol] * len(self.column_names)\n",
    "        start_token = Token(dict(zip(self.column_names, start)))\n",
    "        end_token = Token(dict(zip(self.column_names, end)))\n",
    "        self.start_rows = [start_token] * self.w_size\n",
    "        self.end_rows = [end_token] * self.w_size\n",
    "\n",
    "    def transform(self, sentences, training_step=True):\n",
    "        X_corpus = []\n",
    "        y_corpus = []\n",
    "        for sentence in sentences:\n",
    "            X, y = self._transform_sentence(sentence, training_step)\n",
    "            X_corpus += X\n",
    "            if training_step:\n",
    "                y_corpus += y\n",
    "        return X_corpus, y_corpus\n",
    "\n",
    "    def fit_transform(self, sentences):\n",
    "        self.fit(sentences)\n",
    "        return self.transform(sentences)\n",
    "\n",
    "    def _transform_sentence(self, sentence, training_step=True):\n",
    "        # We extract y\n",
    "        if training_step:\n",
    "            y = [row[self.output] for row in sentence]\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        # We pad the sentence\n",
    "        sentence = self.start_rows + sentence + self.end_rows\n",
    "\n",
    "        # We extract the features\n",
    "        X = list()\n",
    "        for i in range(len(sentence) - 2 * self.w_size):\n",
    "            # x is a row of X\n",
    "            x = list()\n",
    "            # The words in lower case\n",
    "            for j in range(2 * self.w_size + 1):\n",
    "                if self.tolower:\n",
    "                    x.append(sentence[i + j][self.input].lower())\n",
    "                else:\n",
    "                    x.append(sentence[i + j][self.input])\n",
    "            # We represent the feature vector as a dictionary\n",
    "            X.append(dict(zip(self.feature_names, x)))\n",
    "        return X, y\n",
    "\n",
    "    def print_example(self, sentences, id=1968):\n",
    "        \"\"\"\n",
    "        :param corpus:\n",
    "        :param id:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # We print the features to check they match Table 8.1 in my book (second edition)\n",
    "        # We use the training step extraction with the dynamic features\n",
    "        Xs, ys = self._transform_sentence(sentences[id])\n",
    "        print('X for sentence #', id, Xs)\n",
    "        print('y for sentence #', id, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dictorizer = ContextDictorizer()\n",
    "context_dictorizer.fit(train_dict)\n",
    "X_dict, y_cat = context_dictorizer.transform(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dictorizer.print_example(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the $\\mathbf{X}$ Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform the X symbols into numbers\n",
    "dict_vectorizer = DictVectorizer()\n",
    "X_num = dict_vectorizer.fit_transform(X_dict)\n",
    "\n",
    "if SCALER:\n",
    "    # We standardize X_num\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    X = scaler.fit_transform(X_num)\n",
    "else:\n",
    "    X = X_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The POS and the number of different POS\n",
    "pos_list = sorted(list(set(y_cat)))\n",
    "NB_CLASSES = len(pos_list) + 1\n",
    "\n",
    "# We build a part-of-speech index. We keep 0 for unknown symbols in the test set\n",
    "pos_rev_idx = dict(enumerate(pos_list, start=1))\n",
    "pos_idx = {v: k for k, v in pos_rev_idx.items()}\n",
    "\n",
    "# We encode y\n",
    "y = [pos_idx[i] for i in y_cat]\n",
    "print(y_cat[:10])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "if SIMPLE_MODEL:\n",
    "    model.add(layers.Dense(NB_CLASSES,\n",
    "                           input_dim=X.shape[1],\n",
    "                           activation='softmax'))\n",
    "else:\n",
    "    model.add(layers.Dense(NB_CLASSES * 2,\n",
    "                           input_dim=X.shape[1],\n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(NB_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "# model.save('out.model')\n",
    "print('Time:', (time.perf_counter() - start_time) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dict, y_test_cat = context_dictorizer.transform(test_dict)\n",
    "\n",
    "# We transform the symbols into numbers\n",
    "X_test_num = dict_vectorizer.transform(X_test_dict)\n",
    "X_test = scaler.transform(X_test_num)\n",
    "y_test = [pos_idx.get(i, 0) for i in y_test_cat]\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Optimizer', OPTIMIZER, 'Scaler', SCALER, 'Epochs', EPOCHS, 'Batch size', \n",
    "      BATCH_SIZE, 'Simple model', SIMPLE_MODEL, 'Mini corpus', MINI_CORPUS)\n",
    "print('Loss:', test_loss)\n",
    "print('Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
